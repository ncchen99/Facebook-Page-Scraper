import time
import random
import csv
import json
import pickle
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.edge.options import Options as EdgeOptions
from selenium.webdriver.chrome.options import Options as ChromeOptions
import os
import threading


class FacebookPageScraper:
    def __init__(self, email, password, use_edge=True):
        """
        åˆå§‹åŒ–Facebookç²‰çµ²å°ˆé çˆ¬èŸ²
        :param email: Facebookç™»å…¥å¸³è™Ÿ
        :param password: Facebookç™»å…¥å¯†ç¢¼  
        :param use_edge: æ˜¯å¦ä½¿ç”¨Edgeç€è¦½å™¨ï¼ˆæ¨è–¦ï¼Œå› ç‚ºæ™‚é–“æˆ³æ›´å®¹æ˜“æ“·å–ï¼‰
        """
        self.email = email
        self.password = password
        self.use_edge = use_edge
        self.driver = None
        self.scraped_posts = []
        self.stop_scraping = False
        self.auto_save_interval = 5  # æ¯æŠ“å–5ç¯‡è²¼æ–‡è‡ªå‹•ä¿å­˜ä¸€æ¬¡
        self.partial_files = []  # å„²å­˜éƒ¨åˆ†æª”æ¡ˆçš„åˆ—è¡¨
        self.save_callback = None  # ä¿å­˜ç‹€æ…‹å›èª¿å‡½æ•¸
        self.cookie_file = "facebook_cookies.pkl"  # Cookieæª”æ¡ˆè·¯å¾‘
        self.cookie_expiry_days = 7  # Cookieæœ‰æ•ˆæœŸé™ï¼ˆå¤©ï¼‰

    def initialize_driver(self):
        """åˆå§‹åŒ–ç¶²é ç€è¦½å™¨é©…å‹•"""
        try:
            if self.use_edge:
                options = EdgeOptions()
                # ååµæ¸¬è¨­å®š
                options.add_argument(
                    "--disable-blink-features=AutomationControlled")
                options.add_experimental_option(
                    "excludeSwitches", ["enable-automation"])
                options.add_experimental_option(
                    "useAutomationExtension", False)
                options.add_argument(
                    "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59")

                self.driver = webdriver.Edge(options=options)
            else:
                options = ChromeOptions()
                options.add_argument(
                    "--disable-blink-features=AutomationControlled")
                options.add_experimental_option(
                    "excludeSwitches", ["enable-automation"])
                options.add_experimental_option(
                    "useAutomationExtension", False)
                options.add_argument(
                    "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

                # æª¢æŸ¥æ˜¯å¦æœ‰chromedriver
                chromedriver_path = os.path.join(
                    os.getcwd(), "chromedriver-win64", "chromedriver.exe")
                if os.path.exists(chromedriver_path):
                    self.driver = webdriver.Chrome(
                        executable_path=chromedriver_path, options=options)
                else:
                    self.driver = webdriver.Chrome(options=options)

            # ç§»é™¤webdriverç—•è·¡
            self.driver.execute_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            return True

        except Exception as e:
            print(f"ç€è¦½å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False

    def simulate_human_typing(self, element, text):
        """æ¨¡æ“¬äººé¡æ‰“å­—æ¨¡å¼"""
        for char in text:
            if self.stop_scraping:
                break
            element.send_keys(char)
            time.sleep(random.uniform(0.1, 0.3))
            if random.random() < 0.1:
                time.sleep(random.uniform(0.3, 0.7))

    def login(self):
        """ç™»å…¥Facebookï¼ˆæ”¯æ´Cookieå¿«é€Ÿç™»å…¥ï¼‰"""
        try:
            # å…ˆå˜—è©¦ä½¿ç”¨Cookieç™»å…¥
            print("å˜—è©¦ä½¿ç”¨å·²ä¿å­˜çš„ç™»å…¥ç‹€æ…‹...")
            if self.load_cookies():
                # é‡æ–°è¼‰å…¥é é¢ä»¥æ‡‰ç”¨cookies
                self.driver.refresh()
                time.sleep(3)

                # æª¢æŸ¥æ˜¯å¦æˆåŠŸç™»å…¥
                if self.is_logged_in():
                    print("âœ… ä½¿ç”¨Cookieç™»å…¥æˆåŠŸï¼")
                    print("ğŸ”§ Cookieç™»å…¥å¾Œæª¢æŸ¥å½ˆçª—...")
                    time.sleep(1)  # çŸ­æš«ç­‰å¾…
                    self.close_overlay_dialogs()
                    return True
                else:
                    print("Cookieç™»å…¥å¤±æ•—ï¼Œå˜—è©¦å‚³çµ±ç™»å…¥...")

            # å‚³çµ±ç™»å…¥æµç¨‹
            print("æ­£åœ¨å‰å¾€Facebookç™»å…¥é é¢...")
            self.driver.get("https://www.facebook.com/login")

            # ç­‰å¾…ä¸¦å¡«å…¥email
            email_input = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.NAME, "email"))
            )
            self.simulate_human_typing(email_input, self.email)

            # ç­‰å¾…ä¸¦å¡«å…¥å¯†ç¢¼
            password_input = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.NAME, "pass"))
            )
            self.simulate_human_typing(password_input, self.password)

            # é»æ“Šç™»å…¥æŒ‰éˆ•
            login_button = self.driver.find_element(
                By.XPATH, "//button[@type='submit']")
            ActionChains(self.driver)\
                .move_to_element(login_button)\
                .pause(random.uniform(0.2, 0.4))\
                .click()\
                .perform()

            print("ç™»å…¥ä¸­ï¼Œè«‹ç¨ç­‰...")
            time.sleep(15)

            # æª¢æŸ¥æ˜¯å¦æˆåŠŸç™»å…¥
            if "facebook.com" in self.driver.current_url and "login" not in self.driver.current_url:
                print("âœ… å¸³è™Ÿå¯†ç¢¼ç™»å…¥æˆåŠŸï¼")

                # ä¿å­˜cookiesä¾›ä¸‹æ¬¡ä½¿ç”¨
                self.save_cookies()

                # ç«‹å³è™•ç†ç™»å…¥å¾Œçš„å½ˆçª—
                print("ğŸ”§ è™•ç†ç™»å…¥å¾Œå¯èƒ½çš„å½ˆçª—...")
                time.sleep(1)  # çŸ­æš«ç­‰å¾…
                self.close_overlay_dialogs()

                return True
            else:
                print("âŒ ç™»å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥å¸³è™Ÿå¯†ç¢¼")
                return False

        except Exception as e:
            print(f"âŒ ç™»å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def save_cookies(self):
        """ä¿å­˜ç•¶å‰çš„cookiesåˆ°æª”æ¡ˆ"""
        try:
            cookies = self.driver.get_cookies()
            cookie_data = {
                'cookies': cookies,
                'saved_time': datetime.now().isoformat(),
                'email': self.email  # ä¿å­˜å¸³è™Ÿä»¥ä¾¿é©—è­‰
            }

            with open(self.cookie_file, 'wb') as f:
                pickle.dump(cookie_data, f)

            print(f"âœ… Cookieså·²ä¿å­˜åˆ°: {self.cookie_file}")
            if self.save_callback:
                self.save_callback(f"å·²ä¿å­˜ç™»å…¥ç‹€æ…‹ï¼Œä¸‹æ¬¡å¯å¿«é€Ÿç™»å…¥")

            return True

        except Exception as e:
            print(f"ä¿å­˜cookiesæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def load_cookies(self):
        """å¾æª”æ¡ˆè¼‰å…¥cookies"""
        try:
            if not os.path.exists(self.cookie_file):
                print("æ‰¾ä¸åˆ°cookiesæª”æ¡ˆ")
                return False

            with open(self.cookie_file, 'rb') as f:
                cookie_data = pickle.load(f)

            # æª¢æŸ¥cookiesæ˜¯å¦éæœŸ
            saved_time = datetime.fromisoformat(cookie_data['saved_time'])
            if datetime.now() - saved_time > timedelta(days=self.cookie_expiry_days):
                print("Cookieså·²éæœŸï¼Œéœ€è¦é‡æ–°ç™»å…¥")
                os.remove(self.cookie_file)  # åˆªé™¤éæœŸçš„cookies
                return False

            # æª¢æŸ¥å¸³è™Ÿæ˜¯å¦åŒ¹é…
            if cookie_data.get('email') != self.email:
                print("Cookieså¸³è™Ÿä¸åŒ¹é…ç•¶å‰å¸³è™Ÿ")
                return False

            # å…ˆè¨ªå•Facebookä¸»é 
            self.driver.get("https://www.facebook.com")
            time.sleep(2)

            # è¼‰å…¥cookies
            for cookie in cookie_data['cookies']:
                try:
                    self.driver.add_cookie(cookie)
                except Exception as e:
                    print(f"è¼‰å…¥å–®å€‹cookieå¤±æ•—: {e}")
                    continue

            print("âœ… Cookiesè¼‰å…¥æˆåŠŸ")
            return True

        except Exception as e:
            print(f"è¼‰å…¥cookiesæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œåˆªé™¤å•é¡Œcookiesæª”æ¡ˆ
            if os.path.exists(self.cookie_file):
                os.remove(self.cookie_file)
            return False

    def is_logged_in(self):
        """æª¢æŸ¥æ˜¯å¦å·²ç¶“ç™»å…¥"""
        try:
            # æª¢æŸ¥é é¢ä¸Šæ˜¯å¦æœ‰ç™»å…¥æˆåŠŸçš„æ¨™èªŒ
            self.driver.get("https://www.facebook.com")
            time.sleep(3)

            # å¦‚æœé é¢åŒ…å«ç™»å…¥è¡¨å–®ï¼Œèªªæ˜æœªç™»å…¥
            login_elements = self.driver.find_elements(By.NAME, "email")
            if login_elements:
                return False

            # æª¢æŸ¥æ˜¯å¦æœ‰ç”¨æˆ¶èœå–®æˆ–å…¶ä»–ç™»å…¥æˆåŠŸçš„æ¨™èªŒ
            user_menu = self.driver.find_elements(
                By.CSS_SELECTOR, "[data-testid='blue_bar_profile_link']")
            if user_menu:
                return True

            # æª¢æŸ¥ç¶²å€æ˜¯å¦é‚„åœ¨loginé é¢
            if "login" in self.driver.current_url:
                return False

            return True

        except Exception as e:
            print(f"æª¢æŸ¥ç™»å…¥ç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def parse_facebook_time(self, time_string):
        """å°‡Facebookå„ç¨®æ™‚é–“æ ¼å¼è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼ YYYY-MM-DD HH:MM"""
        if not time_string or time_string == "æœªçŸ¥æ™‚é–“":
            return "æœªçŸ¥æ™‚é–“"

        try:
            from datetime import datetime, timedelta
            import re

            current_time = datetime.now()
            time_string = time_string.strip()

            # è™•ç†ç›¸å°æ™‚é–“æ ¼å¼
            # åŒ¹é…ã€ŒXå°æ™‚ã€ã€ã€ŒXåˆ†é˜ã€ç­‰æ ¼å¼
            relative_pattern = r'(\d+)\s*(å°æ™‚|åˆ†é˜|ç§’)'
            relative_match = re.search(relative_pattern, time_string)
            if relative_match:
                number = int(relative_match.group(1))
                unit = relative_match.group(2)

                if unit == "å°æ™‚":
                    target_time = current_time - timedelta(hours=number)
                elif unit == "åˆ†é˜":
                    target_time = current_time - timedelta(minutes=number)
                elif unit == "ç§’":
                    target_time = current_time - timedelta(seconds=number)

                return target_time.strftime("%Y-%m-%d %H:%M")

            # è™•ç†ã€Œæ˜¨å¤©ã€æ ¼å¼
            if "æ˜¨å¤©" in time_string:
                # æå–æ™‚é–“éƒ¨åˆ†ï¼Œä¾‹å¦‚ã€Œæ˜¨å¤©ä¸Šåˆ11:21ã€
                time_pattern = r'(ä¸Šåˆ|ä¸‹åˆ)?(\d{1,2}):(\d{2})'
                time_match = re.search(time_pattern, time_string)

                yesterday = current_time - timedelta(days=1)

                if time_match:
                    period = time_match.group(1)  # ä¸Šåˆ/ä¸‹åˆ
                    hour = int(time_match.group(2))
                    minute = int(time_match.group(3))

                    # è™•ç†12å°æ™‚åˆ¶
                    if period == "ä¸‹åˆ" and hour != 12:
                        hour += 12
                    elif period == "ä¸Šåˆ" and hour == 12:
                        hour = 0

                    target_time = yesterday.replace(
                        hour=hour, minute=minute, second=0, microsecond=0)
                else:
                    # å¦‚æœæ²’æœ‰å…·é«”æ™‚é–“ï¼Œè¨­ç‚ºæ˜¨å¤©åŒä¸€æ™‚åˆ»
                    target_time = yesterday

                return target_time.strftime("%Y-%m-%d %H:%M")

            # è™•ç†ã€ŒXæœˆXæ—¥ã€æ ¼å¼ï¼ˆç•¶å¹´ï¼‰
            month_day_pattern = r'(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\s*(ä¸Šåˆ|ä¸‹åˆ)?(\d{1,2}):(\d{2}))?'
            month_day_match = re.search(month_day_pattern, time_string)
            if month_day_match:
                month = int(month_day_match.group(1))
                day = int(month_day_match.group(2))
                period = month_day_match.group(3)  # ä¸Šåˆ/ä¸‹åˆ
                hour = int(month_day_match.group(
                    4)) if month_day_match.group(4) else 0
                minute = int(month_day_match.group(
                    5)) if month_day_match.group(5) else 0

                # è™•ç†12å°æ™‚åˆ¶
                if period == "ä¸‹åˆ" and hour != 12:
                    hour += 12
                elif period == "ä¸Šåˆ" and hour == 12:
                    hour = 0

                # åˆ¤æ–·å¹´ä»½ï¼ˆå¦‚æœæœˆä»½å¤§æ–¼ç•¶å‰æœˆä»½ï¼Œå¯èƒ½æ˜¯å»å¹´ï¼‰
                year = current_time.year
                if month > current_time.month or (month == current_time.month and day > current_time.day):
                    year = current_time.year - 1

                target_time = datetime(year, month, day, hour, minute)
                return target_time.strftime("%Y-%m-%d %H:%M")

            # è™•ç†ã€ŒYYYYå¹´XæœˆXæ—¥ã€æ ¼å¼
            full_date_pattern = r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\s*(ä¸Šåˆ|ä¸‹åˆ)?(\d{1,2}):(\d{2}))?'
            full_date_match = re.search(full_date_pattern, time_string)
            if full_date_match:
                year = int(full_date_match.group(1))
                month = int(full_date_match.group(2))
                day = int(full_date_match.group(3))
                period = full_date_match.group(4)  # ä¸Šåˆ/ä¸‹åˆ
                hour = int(full_date_match.group(
                    5)) if full_date_match.group(5) else 0
                minute = int(full_date_match.group(
                    6)) if full_date_match.group(6) else 0

                # è™•ç†12å°æ™‚åˆ¶
                if period == "ä¸‹åˆ" and hour != 12:
                    hour += 12
                elif period == "ä¸Šåˆ" and hour == 12:
                    hour = 0

                target_time = datetime(year, month, day, hour, minute)
                return target_time.strftime("%Y-%m-%d %H:%M")

            # è™•ç†ç´”æ™‚é–“æ ¼å¼ã€Œä¸Šåˆ11:21ã€æˆ–ã€Œä¸‹åˆ2:30ã€ï¼ˆç•¶æ—¥ï¼‰
            time_only_pattern = r'(ä¸Šåˆ|ä¸‹åˆ)(\d{1,2}):(\d{2})'
            time_only_match = re.search(time_only_pattern, time_string)
            if time_only_match:
                period = time_only_match.group(1)
                hour = int(time_only_match.group(2))
                minute = int(time_only_match.group(3))

                # è™•ç†12å°æ™‚åˆ¶
                if period == "ä¸‹åˆ" and hour != 12:
                    hour += 12
                elif period == "ä¸Šåˆ" and hour == 12:
                    hour = 0

                target_time = current_time.replace(
                    hour=hour, minute=minute, second=0, microsecond=0)
                return target_time.strftime("%Y-%m-%d %H:%M")

            # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›åŸå§‹å­—ä¸²
            print(f"âš ï¸ ç„¡æ³•è§£ææ™‚é–“æ ¼å¼: {time_string}")
            return time_string

        except Exception as e:
            print(f"è§£ææ™‚é–“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}, åŸå§‹æ™‚é–“: {time_string}")
            return time_string

    def clean_date_string(self, date_text):
        """æ¸…ç†æ—¥æœŸå­—ä¸²ï¼Œç§»é™¤å¤šé¤˜å­—ç¬¦å’Œåˆ†äº«å°è±¡è³‡è¨Šï¼Œç„¶å¾Œçµ±ä¸€æ ¼å¼"""
        if not date_text:
            return "æœªçŸ¥æ™‚é–“"

        try:
            # ç§»é™¤åˆ†äº«å°è±¡åŠä¹‹å¾Œçš„å…§å®¹
            if "åˆ†äº«å°è±¡ï¼š" in date_text:
                date_text = date_text.split("åˆ†äº«å°è±¡ï¼š")[0]
            if "Â·" in date_text:
                date_text = date_text.split("Â·")[0]

            # ç§»é™¤å¤šé¤˜çš„çŸ­æ©«ç·šå’Œç©ºæ ¼
            cleaned = re.sub(r'-+', '', date_text)  # ç§»é™¤æ‰€æœ‰çŸ­æ©«ç·š
            cleaned = re.sub(r'\s+', ' ', cleaned)  # å°‡å¤šå€‹ç©ºæ ¼åˆä½µç‚ºä¸€å€‹
            cleaned = cleaned.strip()  # ç§»é™¤é¦–å°¾ç©ºæ ¼

            # å¦‚æœæ¸…ç†å¾Œçš„å­—ä¸²å¤ªçŸ­ï¼Œè¿”å›åŸå§‹å­—ä¸²
            if len(cleaned) < 3:
                return date_text.strip()

            # ä½¿ç”¨æ–°çš„æ™‚é–“è§£æå‡½æ•¸çµ±ä¸€æ ¼å¼
            standardized_time = self.parse_facebook_time(cleaned)
            return standardized_time

        except Exception as e:
            print(f"æ¸…ç†æ—¥æœŸå­—ä¸²æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return date_text if date_text else "æœªçŸ¥æ™‚é–“"

    def close_overlay_dialogs(self):
        """é—œé–‰ç™»å…¥å¾Œå¯èƒ½å‡ºç¾çš„é®è”½å½ˆçª—"""
        try:
            print("ğŸ” æª¢æŸ¥æ˜¯å¦æœ‰é®è”½å½ˆçª—éœ€è¦é—œé–‰...")

            # ç¸®çŸ­ç­‰å¾…æ™‚é–“ï¼Œæ›´å¿«é€Ÿæª¢æŸ¥
            time.sleep(1.5)

            # å¯èƒ½çš„é—œé–‰æŒ‰éˆ•é¸æ“‡å™¨
            close_selectors = [
                "div[aria-label='é—œé–‰']",
                "div[aria-label='Close']",
                "button[aria-label='é—œé–‰']",
                "button[aria-label='Close']",
                "div[role='button'][aria-label='é—œé–‰']",
                "div[role='button'][aria-label='Close']",
                "[data-testid='close-button']",
                "div.x92rtbv.x10slt7e.x1c4vz4f.x2lah0s",  # Facebookå¸¸è¦‹çš„é—œé–‰æŒ‰éˆ•æ¨£å¼
                "svg[aria-label='é—œé–‰']",
                "svg[aria-label='Close']"
            ]

            closed_count = 0

            for selector in close_selectors:
                try:
                    # ä½¿ç”¨æ›´çŸ­çš„ç­‰å¾…æ™‚é–“
                    close_buttons = WebDriverWait(self.driver, 1).until(
                        EC.presence_of_all_elements_located(
                            (By.CSS_SELECTOR, selector))
                    )

                    for button in close_buttons:
                        if button.is_displayed() and button.is_enabled():
                            print(f"âœ… ç™¼ç¾ä¸¦é»æ“Šé—œé–‰æŒ‰éˆ•: {selector}")
                            ActionChains(self.driver)\
                                .move_to_element(button)\
                                .pause(0.3)\
                                .click()\
                                .perform()
                            closed_count += 1
                            time.sleep(0.5)  # ç¸®çŸ­ç­‰å¾…æ™‚é–“
                            break
                    else:
                        continue
                    break

                except:
                    # å¦‚æœæ‰¾ä¸åˆ°è©²é¸æ“‡å™¨å°±ç¹¼çºŒå˜—è©¦ä¸‹ä¸€å€‹
                    continue

            # æ›´å¼·æ•ˆçš„å½ˆçª—é—œé–‰æ©Ÿåˆ¶
            try:
                # æ–¹æ³•1ï¼šç›´æ¥é»æ“Šbodyå…ƒç´ ï¼ˆæ›´å¯é çš„æ–¹å¼ï¼‰
                body = self.driver.find_element(By.TAG_NAME, "body")
                ActionChains(self.driver).move_to_element(
                    body).click().perform()
                time.sleep(0.5)
                print("ğŸ–±ï¸ å·²é»æ“Šbodyå…ƒç´ ï¼Œå˜—è©¦é—œé–‰å½ˆçª—")

                # æ–¹æ³•2ï¼šä½¿ç”¨JavaScripté»æ“Šç©ºç™½å€åŸŸ
                self.driver.execute_script("""
                    // é»æ“Šç©ºç™½å€åŸŸ
                    document.body.click();
                    
                    // ç™¼é€å¤šå€‹ESCéµäº‹ä»¶
                    for(let i = 0; i < 3; i++) {
                        document.dispatchEvent(new KeyboardEvent('keydown', {key: 'Escape', bubbles: true}));
                        document.dispatchEvent(new KeyboardEvent('keyup', {key: 'Escape', bubbles: true}));
                    }
                    
                    // å˜—è©¦é—œé–‰å¯èƒ½å­˜åœ¨çš„modal
                    var modals = document.querySelectorAll('[role="dialog"], [data-testid="modal"], .modal');
                    modals.forEach(function(modal) {
                        if(modal.style) {
                            modal.style.display = 'none';
                        }
                    });
                """)
                time.sleep(0.5)
                print("ğŸ”§ å·²åŸ·è¡ŒJavaScriptå½ˆçª—é—œé–‰è…³æœ¬")

                # æ–¹æ³•3ï¼šActionChainsç™¼é€ESCéµ
                for i in range(2):
                    try:
                        ActionChains(self.driver).send_keys(
                            Keys.ESCAPE).perform()
                        time.sleep(0.3)
                    except:
                        pass

                print("âŒ¨ï¸ å·²å˜—è©¦å¤šç¨®æ–¹å¼é—œé–‰å½ˆçª—")

            except Exception as e:
                print(f"âš ï¸ å½ˆçª—é—œé–‰æ“ä½œå¤±æ•—: {e}")
                pass

            # å¿«é€Ÿæª¢æŸ¥æ˜¯å¦é‚„æœ‰é®è”½å±¤
            try:
                overlay_count = self.driver.execute_script("""
                    var overlays = document.querySelectorAll('[role="dialog"], [data-testid="modal"], .modal, .overlay');
                    var visibleCount = 0;
                    overlays.forEach(function(overlay) {
                        var style = window.getComputedStyle(overlay);
                        if(style.display !== 'none' && style.visibility !== 'hidden' && style.opacity !== '0') {
                            visibleCount++;
                        }
                    });
                    return visibleCount;
                """)

                if overlay_count > 0:
                    print(f"âš ï¸ ä»æª¢æ¸¬åˆ° {overlay_count} å€‹å¯èƒ½çš„é®è”½å±¤")
                else:
                    print("âœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„é®è”½å±¤")

            except:
                pass

            if closed_count > 0:
                print(f"âœ… æˆåŠŸé—œé–‰äº† {closed_count} å€‹å½ˆçª—")
            else:
                print("â„¹ï¸ æœªç™¼ç¾æ˜é¡¯å½ˆçª—ï¼Œå·²åŸ·è¡Œé é˜²æ€§é—œé–‰æ“ä½œ")

            print("âœ… å½ˆçª—æª¢æŸ¥å®Œæˆ")

        except Exception as e:
            print(f"âŒ é—œé–‰é®è”½å½ˆçª—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def navigate_to_page(self, page_url):
        """å‰å¾€æŒ‡å®šçš„ç²‰çµ²å°ˆé """
        try:
            print(f"ğŸŒ æ­£åœ¨å‰å¾€ç²‰çµ²å°ˆé : {page_url}")
            self.driver.get(page_url)
            time.sleep(2)  # ç¸®çŸ­åŸºæœ¬ç­‰å¾…æ™‚é–“

            # ç«‹å³æª¢æŸ¥ä¸¦é—œé–‰å¯èƒ½çš„å½ˆçª—ï¼ˆä¸ç­‰å¾…å¤ªä¹…ï¼‰
            print("ğŸ”§ ç«‹å³æª¢æŸ¥é é¢å½ˆçª—...")
            self.close_overlay_dialogs()

            # å†æ¬¡ç¢ºèªé é¢è¼‰å…¥
            time.sleep(1)

            return True
        except Exception as e:
            print(f"âŒ å‰å¾€ç²‰çµ²å°ˆé å¤±æ•—: {e}")
            return False

    def slow_scroll(self, step=100):
        """éå¸¸ç·©æ…¢æ»¾å‹•é é¢ä»¥è¼‰å…¥æ›´å¤šè²¼æ–‡"""
        self.driver.execute_script(f"window.scrollBy(0, {step});")
        time.sleep(0.8)  # è¼ƒçŸ­çš„æ»¾å‹•é–“éš”

    def fast_scroll_with_realtime_extract(self, total_distance=1000, step=100, all_posts=None):
        """å¿«é€Ÿæ»¾å‹•ä¸¦å¯¦æ™‚æŠ“å–è²¼æ–‡å…§å®¹ï¼ˆé©æ‡‰Facebookçš„å³æ™‚è¼‰å…¥æ©Ÿåˆ¶ï¼‰"""
        print(f"ğŸ”„ é–‹å§‹å¿«é€Ÿæ»¾å‹• {total_distance}pxï¼ˆæ­¥é•· {step}pxï¼‰ï¼Œå¯¦æ™‚æŠ“å–è²¼æ–‡å…§å®¹...")

        total_clicked = 0
        current_posts = all_posts if all_posts else []

        steps = total_distance // step
        for i in range(steps):
            if self.stop_scraping:
                break

            # æ»¾å‹•ä¸€æ­¥
            self.driver.execute_script(f"window.scrollBy(0, {step});")
            time.sleep(0.3)  # å¿«é€Ÿç­‰å¾…è¼‰å…¥

            # æ¯æ­¥éƒ½é»æ“Šã€ŒæŸ¥çœ‹æ›´å¤šã€ä¸¦å¯¦æ™‚æŠ“å–å…§å®¹
            new_clicks, current_posts = self.quick_click_see_more(
                current_posts)
            total_clicked += new_clicks

            # å¦‚æœæ²’æœ‰é»æ“Šï¼Œä¹Ÿè¦æ¯æ­¥æŠ“å–ç•¶å‰å¯è¦‹çš„è²¼æ–‡å…§å®¹
            if new_clicks == 0 and i % 1 == 0:  # æ²’é»æ“Šæ™‚æ¯æ­¥éƒ½æŠ“å–
                fresh_posts = self.extract_posts_with_bs()
                # æ™ºæ…§åˆä½µï¼Œä¿ç•™æœ€å®Œæ•´çš„å…§å®¹
                current_posts = self.smart_merge_posts(
                    current_posts, fresh_posts)

            # çŸ­æš«é–“éš”ï¼Œä¿æŒé«˜æ•ˆç‡
            time.sleep(random.uniform(0.2, 0.4))

        if total_clicked > 0:
            print(
                f"âœ… æœ¬è¼ªæ»¾å‹•ç¸½å…±è™•ç†äº† {total_clicked} å€‹ã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—æ¨™ç±¤ï¼Œå¯¦æ™‚æ›´æ–°äº† {len(current_posts)} ç¯‡è²¼æ–‡")

        return total_clicked, current_posts

    # å‘å¾Œå…¼å®¹æ€§åˆ¥å
    def slow_scroll_with_see_more(self, total_distance=1000, step=100, all_posts=None):
        """å‘å¾Œå…¼å®¹æ€§æ–¹æ³•ï¼Œå¯¦éš›èª¿ç”¨fast_scroll_with_realtime_extract"""
        return self.fast_scroll_with_realtime_extract(total_distance, step, all_posts)

    def quick_click_see_more(self, current_posts=None):
        """å¿«é€Ÿé»æ“Šã€ŒæŸ¥çœ‹æ›´å¤šã€æŒ‰éˆ•ä¸¦æŠ“å–æ›´æ–°å…§å®¹"""
        try:
            script = """
            var clicked = 0;
            var allElements = document.querySelectorAll('span, div, a');
            
            for(var i = 0; i < allElements.length && clicked < 20; i++) {
                var element = allElements[i];
                var text = (element.innerText || element.textContent || '').trim();
                
                if(text === 'æŸ¥çœ‹æ›´å¤š' || text === 'See More' || text === 'See more') {
                    try {
                        var rect = element.getBoundingClientRect();
                        var isVisible = rect.width > 0 && rect.height > 0 && 
                                      rect.top >= 0 && rect.bottom <= window.innerHeight;
                        
                        if(isVisible) {
                            element.click();
                            clicked++;
                        }
                    } catch(e) {}
                }
            }
            return clicked;
            """

            clicked_count = self.driver.execute_script(script)

            # å¦‚æœæœ‰é»æ“Šï¼Œç­‰å¾…0.5ç§’å¾ŒæŠ“å–æ›´æ–°å…§å®¹
            if clicked_count > 0:
                time.sleep(0.5)
                fresh_posts = self.extract_posts_with_bs()
                if current_posts is not None:
                    # æ™ºæ…§åˆä½µä»¥ç²å¾—æœ€æ–°çš„å®Œæ•´å…§å®¹
                    updated_posts = self.smart_merge_posts(
                        current_posts, fresh_posts)
                    return clicked_count, updated_posts
                else:
                    return clicked_count, fresh_posts

            return clicked_count, current_posts if current_posts is not None else []

        except Exception as e:
            return 0, current_posts if current_posts is not None else []

    def smart_click_see_more_buttons(self, all_posts=None):
        """ç°¡åŒ–ç‰ˆï¼šç›´æ¥é»æ“Šæ‰€æœ‰å¯è¦‹çš„ã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—æ¨™ç±¤ï¼Œä¸¦ç«‹å³æŠ“å–æ›´æ–°å…§å®¹"""
        try:
            clicked_count = 0
            updated_posts = all_posts if all_posts else []

            # æœ€ç°¡åŒ–çš„JavaScriptï¼šæ‰¾åˆ°ã€ŒæŸ¥çœ‹æ›´å¤šã€å°±ç›´æ¥é»æ“Š
            script = """
            var clicked = 0;
            
            // å°‹æ‰¾æ‰€æœ‰æ–‡å­—å…ƒç´ 
            var allElements = document.querySelectorAll('span, div, a');
            
            for(var i = 0; i < allElements.length && clicked < 50; i++) {
                var element = allElements[i];
                var text = (element.innerText || element.textContent || '').trim();
                
                // ç²¾ç¢ºåŒ¹é…ã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—
                if(text === 'æŸ¥çœ‹æ›´å¤š' || text === 'See More' || text === 'See more') {
                    try {
                        // æª¢æŸ¥å…ƒç´ æ˜¯å¦å¯è¦‹
                        var rect = element.getBoundingClientRect();
                        var isVisible = rect.width > 0 && rect.height > 0;
                        
                        if(isVisible) {
                            // ç›´æ¥é»æ“Š
                            element.click();
                            console.log('æˆåŠŸé»æ“ŠæŸ¥çœ‹æ›´å¤š:', text);
                            clicked++;
                        }
                    } catch(e) {
                        console.log('é»æ“Šå¤±æ•—:', e);
                    }
                }
            }
            
            return clicked;
            """

            # åŸ·è¡ŒJavaScriptè…³æœ¬
            clicked_count = self.driver.execute_script(script)

            if clicked_count > 0:
                print(f"âœ… é»æ“Šäº† {clicked_count} å€‹ã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—æ¨™ç±¤")

                # ç­‰å¾…æ›´é•·æ™‚é–“ä¸¦é€²è¡Œå¤šæ¬¡é©—è­‰
                print("â³ ç­‰å¾…å…§å®¹å®Œå…¨å±•é–‹ä¸¦é©—è­‰...")

                # é€²è¡Œå¤šè¼ªç­‰å¾…å’Œé©—è­‰
                best_posts = None
                for attempt in range(3):  # æœ€å¤š3æ¬¡é©—è­‰
                    time.sleep(1.5)  # æ¯æ¬¡ç­‰å¾…1.5ç§’

                    current_posts = self.extract_posts_with_bs()

                    # è¨ˆç®—ç•¶å‰é€™æ‰¹å…§å®¹ä¸­æœ‰å¤šå°‘ä»åŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€
                    truncated_count = sum(1 for post in current_posts
                                          if 'æŸ¥çœ‹æ›´å¤š' in post.get('post_text', '') or
                                          'See More' in post.get('post_text', '') or
                                          'See more' in post.get('post_text', ''))

                    print(
                        f"ğŸ” ç¬¬ {attempt + 1} æ¬¡é©—è­‰ï¼š{len(current_posts)} ç¯‡è²¼æ–‡ï¼Œ{truncated_count} ç¯‡ä»æˆªæ–·")

                    # å¦‚æœé€™æ¬¡çš„çµæœæ›´å¥½ï¼ˆæˆªæ–·å…§å®¹æ›´å°‘ï¼‰ï¼Œå°±ä¿å­˜
                    if best_posts is None or truncated_count < sum(1 for post in best_posts
                                                                   if 'æŸ¥çœ‹æ›´å¤š' in post.get('post_text', '') or
                                                                   'See More' in post.get('post_text', '') or
                                                                   'See more' in post.get('post_text', '')):
                        best_posts = current_posts
                        if truncated_count == 0:
                            print(f"âœ… ç¬¬ {attempt + 1} æ¬¡é©—è­‰ï¼šæ‰€æœ‰å…§å®¹å·²å®Œå…¨å±•é–‹ï¼")
                            break

                new_posts = best_posts if best_posts else self.extract_posts_with_bs()

                # é€²è¡Œæ™ºæ…§åˆä½µ
                if all_posts is not None:
                    updated_posts = self.smart_merge_posts(
                        all_posts, new_posts)
                    print(f"ğŸ“Š åˆä½µå®Œæˆï¼š{len(updated_posts)} ç¯‡è²¼æ–‡")
                else:
                    updated_posts = new_posts

            return clicked_count, updated_posts

        except Exception as e:
            print(f"âŒ é»æ“Šã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—æ¨™ç±¤æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0, updated_posts

    def click_see_more_buttons(self):
        """é»æ“Šé é¢ä¸Šå¯è¦‹çš„ã€ŒæŸ¥çœ‹æ›´å¤šã€æŒ‰éˆ•ä¾†å±•é–‹å®Œæ•´å…§å®¹ï¼ˆä¿ç•™èˆŠæ–¹æ³•ä»¥ä¾¿å‘å¾Œå…¼å®¹ï¼‰"""
        clicks, _ = self.smart_click_see_more_buttons()
        return clicks

    def extract_posts_with_bs(self):
        """ä½¿ç”¨BeautifulSoupæ“·å–è²¼æ–‡è³‡æ–™"""
        try:
            # æ³¨æ„ï¼šã€ŒæŸ¥çœ‹æ›´å¤šã€æŒ‰éˆ•çš„é»æ“Šç¾åœ¨åœ¨æ»¾å‹•éç¨‹ä¸­é€²è¡Œï¼Œé€™è£¡ä¸å†é‡è¤‡åŸ·è¡Œ
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, "html.parser")
            posts_data = []

            # å°‹æ‰¾è²¼æ–‡å®¹å™¨
            posts = soup.find_all("div", {"class": "x1n2onr6 x1ja2u2z"})

            for post in posts:
                if self.stop_scraping:
                    break

                try:
                    # æ“·å–è²¼æ–‡æ–‡å­—å…§å®¹
                    message_elements = post.find_all(
                        "div", {"data-ad-preview": "message"})
                    post_text = " ".join([msg.get_text(strip=True)
                                         for msg in message_elements])

                    # æ“·å–æŒ‰è®šæ•¸
                    likes_element = post.select_one(
                        "span.xt0b8zv.x1jx94hy.xrbpyxo.xl423tq > span > span")
                    likes = likes_element.get_text(
                        strip=True) if likes_element else "0"

                    # æ“·å–ç•™è¨€æ•¸
                    comments_element = post.select(
                        "div > div > span > div > div > div > span > span.html-span")
                    comments = comments_element[0].text if comments_element else "0"

                    # æ“·å–åˆ†äº«æ•¸
                    shares_element = post.select(
                        "div > div > span > div > div > div > span > span.html-span")
                    shares = shares_element[1].text if len(
                        shares_element) > 1 else "0"

                    # æ“·å–è²¼æ–‡æ™‚é–“ - ä½¿ç”¨ dir="ltr" æ¨™ç±¤ï¼Œç¬¬äºŒå€‹å…ƒç´ æ˜¯æ™‚é–“
                    post_time = "æœªçŸ¥æ™‚é–“"
                    try:
                        # æ‰¾åˆ°æ‰€æœ‰ dir="ltr" çš„å…ƒç´ 
                        ltr_elements = post.find_all(attrs={"dir": "ltr"})
                        if len(ltr_elements) >= 2:
                            # ç¬¬äºŒå€‹ dir="ltr" å…ƒç´ é€šå¸¸æ˜¯æ™‚é–“
                            time_element = ltr_elements[1]
                            candidate_time = time_element.get_text(strip=True)
                            # é©—è­‰æ˜¯å¦ç‚ºåˆç†çš„æ™‚é–“æ ¼å¼
                            if candidate_time and len(candidate_time) < 100:
                                post_time = self.clean_date_string(
                                    candidate_time)

                        # å¦‚æœé€šé dir="ltr" æ²’æ‰¾åˆ°åˆé©çš„æ™‚é–“ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
                        if post_time == "æœªçŸ¥æ™‚é–“":
                            time_selectors = [
                                "a[role='link'] span[dir='ltr']",
                                "div.xu06os2.x1ok221b > span > div > span > span > a > span",
                                "span[dir='ltr']",
                                "time",
                                "[data-testid='story-subtitle'] span"
                            ]

                            for selector in time_selectors:
                                try:
                                    time_elem = post.select_one(selector)
                                    if time_elem:
                                        candidate_time = time_elem.get_text(
                                            strip=True)
                                        # æª¢æŸ¥æ˜¯å¦åƒæ™‚é–“æ ¼å¼ï¼ˆåŒ…å«æ•¸å­—ä¸”ä¸å¤ªé•·ï¼‰
                                        if candidate_time and len(candidate_time) < 50 and any(char.isdigit() for char in candidate_time):
                                            post_time = self.clean_date_string(
                                                candidate_time)
                                            break
                                except:
                                    continue
                    except Exception as e:
                        print(f"æ“·å–æ™‚é–“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        post_time = "æœªçŸ¥æ™‚é–“"

                    # æ“·å–è²¼æ–‡é€£çµ
                    link_element = post.select_one(
                        "div.xu06os2.x1ok221b > span > div > span > span > a")
                    post_url = link_element.get('href') if link_element else ""
                    if post_url and not post_url.startswith('http'):
                        post_url = "https://www.facebook.com" + post_url

                    # åªåŠ å…¥æœ‰å…§å®¹çš„è²¼æ–‡
                    if post_text.strip() or likes != "0" or comments != "0":
                        posts_data.append({
                            "post_text": post_text,
                            "likes": likes,
                            "comments": comments,
                            "shares": shares,
                            "post_time": post_time,
                            "post_url": post_url,
                            "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        })

                except Exception as e:
                    print(f"æ“·å–å–®ä¸€è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue

            return posts_data

        except Exception as e:
            print(f"æ“·å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def remove_duplicates(self, data_list):
        """ç§»é™¤é‡è¤‡çš„è²¼æ–‡"""
        seen = set()
        unique_data = []
        for data in data_list:
            # ä½¿ç”¨è²¼æ–‡æ–‡å­—å’Œæ™‚é–“ä½œç‚ºå”¯ä¸€è­˜åˆ¥
            # æ”¯æ´å­—å…¸å’Œç‰©ä»¶å…©ç¨®æ ¼å¼
            if isinstance(data, dict):
                identifier = (data.get('post_text', ''),
                              data.get('post_time', ''))
            else:
                identifier = (getattr(data, 'post_text', ''),
                              getattr(data, 'post_time', ''))

            if identifier not in seen and identifier != ('', '') and identifier != ('æœªçŸ¥æ™‚é–“', ''):
                seen.add(identifier)
                unique_data.append(data)
        return unique_data

    def smart_merge_posts(self, old_posts, new_posts):
        """æ™ºæ…§åˆä½µè²¼æ–‡ï¼šå¦‚æœèˆŠè²¼æ–‡åŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€ï¼Œç”¨æ–°å…§å®¹æ›¿æ›"""
        if not old_posts:
            return self.remove_duplicates(new_posts)

        if not new_posts:
            return old_posts

        print(f"ğŸ”„ æ™ºæ…§åˆä½µè²¼æ–‡ï¼šèˆŠ {len(old_posts)} ç¯‡ + æ–° {len(new_posts)} ç¯‡")

        # å»ºç«‹æ–°è²¼æ–‡çš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸ï¼ˆä½¿ç”¨å‰150å­—ç¬¦å’Œæ™‚é–“ä½œç‚ºkeyï¼‰
        new_posts_dict = {}
        for post in new_posts:
            text_key = post.get('post_text', '')[:150].strip()
            time_key = post.get('post_time', '')
            combined_key = (text_key, time_key)
            if text_key:
                new_posts_dict[combined_key] = post

        merged_posts = []
        replaced_count = 0

        # æª¢æŸ¥æ¯å€‹èˆŠè²¼æ–‡
        for old_post in old_posts:
            old_text = old_post.get('post_text', '').strip()
            old_text_key = old_text[:150].strip()
            old_time_key = old_post.get('post_time', '')
            old_combined_key = (old_text_key, old_time_key)

            # æª¢æŸ¥èˆŠè²¼æ–‡æ˜¯å¦åŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€ï¼ˆè¡¨ç¤ºå…§å®¹è¢«æˆªæ–·ï¼‰
            has_see_more = (
                'æŸ¥çœ‹æ›´å¤š' in old_text or
                'See More' in old_text or
                'See more' in old_text
            )

            if has_see_more and old_combined_key in new_posts_dict:
                # ç”¨æ–°çš„å®Œæ•´å…§å®¹æ›¿æ›èˆŠçš„æˆªæ–·å…§å®¹
                new_post = new_posts_dict[old_combined_key]
                new_text = new_post.get('post_text', '').strip()

                # æª¢æŸ¥æ–°å…§å®¹æ˜¯å¦ä¹ŸåŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€
                new_has_see_more = (
                    'æŸ¥çœ‹æ›´å¤š' in new_text or
                    'See More' in new_text or
                    'See more' in new_text
                )

                # å¦‚æœæ–°æ–‡ç« åŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€ï¼Œä¿ç•™èˆŠå…§å®¹ä¸é€²è¡Œæ›´æ–°
                if new_has_see_more:
                    print(f"âš ï¸ æ–°æ–‡ç« ä»åŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€ï¼Œä¿ç•™èˆŠå…§å®¹ä¸æ›´æ–°")
                    merged_posts.append(old_post)
                    # å¾æ–°è²¼æ–‡å­—å…¸ä¸­ç§»é™¤ï¼Œé¿å…é‡è¤‡æ·»åŠ 
                    del new_posts_dict[old_combined_key]
                elif len(new_text) > len(old_text):
                    print(f"âœ… æ›¿æ›æˆªæ–·å…§å®¹ï¼š{len(old_text)} â†’ {len(new_text)} å­—ç¬¦")
                    merged_posts.append(new_post)
                    replaced_count += 1
                    # å¾æ–°è²¼æ–‡å­—å…¸ä¸­ç§»é™¤ï¼Œé¿å…é‡è¤‡æ·»åŠ 
                    del new_posts_dict[old_combined_key]
                else:
                    # æ–°å…§å®¹æ²’æœ‰æ”¹å–„ï¼Œä¿ç•™èˆŠå…§å®¹
                    merged_posts.append(old_post)
            else:
                # èˆŠè²¼æ–‡æ²’æœ‰ã€ŒæŸ¥çœ‹æ›´å¤šã€æˆ–æ²’æœ‰å°æ‡‰çš„æ–°å…§å®¹ï¼Œä¿ç•™åŸæ¨£
                merged_posts.append(old_post)
                # å¦‚æœæ–°è²¼æ–‡ä¸­æœ‰ç›¸åŒçš„ï¼Œç§»é™¤ä»¥é¿å…é‡è¤‡
                if old_combined_key in new_posts_dict:
                    del new_posts_dict[old_combined_key]

        # æ·»åŠ å‰©é¤˜çš„æ–°è²¼æ–‡
        for remaining_post in new_posts_dict.values():
            merged_posts.append(remaining_post)

        # æœ€çµ‚å»é‡
        final_posts = self.remove_duplicates(merged_posts)

        if replaced_count > 0:
            print(f"ğŸ”„ æˆåŠŸæ›¿æ›äº† {replaced_count} å€‹æˆªæ–·è²¼æ–‡ç‚ºå®Œæ•´å…§å®¹")

        return final_posts

    def scrape_posts(self, max_posts, progress_callback=None):
        """çˆ¬å–æŒ‡å®šæ•¸é‡çš„è²¼æ–‡ï¼Œä½¿ç”¨æ™ºæ…§æ»¾å‹•ç­–ç•¥ç¢ºä¿æ‰€æœ‰ã€ŒæŸ¥çœ‹æ›´å¤šã€éƒ½è¢«é»æ“Š"""
        all_posts = []
        scroll_attempts = 0
        max_scroll_attempts = 50  # æœ€å¤§æ»¾å‹•æ¬¡æ•¸
        batch_number = 1
        last_save_count = 0
        total_see_more_clicks = 0  # çµ±è¨ˆç¸½é»æ“Šæ•¸é‡

        print(
            f"ğŸš€ é–‹å§‹é«˜æ•ˆçˆ¬å–è²¼æ–‡ï¼Œç›®æ¨™: {max_posts} ç¯‡ï¼ˆæ¯ {self.auto_save_interval} ç¯‡è‡ªå‹•ä¿å­˜ï¼‰")
        print("âš¡ æ¡ç”¨å¿«é€Ÿæ»¾å‹•ç­–ç•¥ï¼Œå¯¦æ™‚æŠ“å–ä¸¦å±•é–‹è²¼æ–‡å…§å®¹")

        # åˆå§‹åŠ è¼‰ä¸¦æŠ“å–
        print("ğŸ” åˆå§‹åŠ è¼‰ï¼šæŠ“å–ç•¶å‰é é¢è²¼æ–‡...")
        all_posts = self.extract_posts_with_bs()
        initial_clicks, all_posts = self.quick_click_see_more(all_posts)
        total_see_more_clicks += initial_clicks
        if initial_clicks > 0:
            print(f"âœ… åˆå§‹åŠ è¼‰é»æ“Šäº† {initial_clicks} å€‹ã€ŒæŸ¥çœ‹æ›´å¤šã€ï¼Œå·²æ›´æ–°å…§å®¹")

        while scroll_attempts < max_scroll_attempts and not self.stop_scraping:
            # è¨ˆç®—ç•¶å‰å®Œæ•´è²¼æ–‡æ•¸é‡ï¼ˆä¸åŒ…å«ã€ŒæŸ¥çœ‹æ›´å¤šã€çš„è²¼æ–‡ï¼‰
            complete_posts = [
                post for post in all_posts
                if not ('æŸ¥çœ‹æ›´å¤š' in post.get('post_text', '') or
                        'See More' in post.get('post_text', '') or
                        'See more' in post.get('post_text', ''))
            ]

            # å¦‚æœå·²ç¶“ç²å¾—è¶³å¤ çš„å®Œæ•´è²¼æ–‡ï¼Œå°±åœæ­¢
            if len(complete_posts) >= max_posts:
                print(f"ğŸ¯ å·²ç²å¾— {len(complete_posts)} ç¯‡å®Œæ•´è²¼æ–‡ï¼Œé”åˆ°ç›®æ¨™ï¼")
                break
            # ä½¿ç”¨é«˜æ•ˆæ»¾å‹•ç­–ç•¥ï¼šå¿«é€Ÿæ»¾å‹•ä¸¦å¯¦æ™‚æŠ“å–æ›´æ–°å…§å®¹
            scroll_distance = 600 + random.randint(-100, 100)  # æ¸›å°‘æ»¾å‹•è·é›¢æé«˜æ•ˆç‡
            step_size = 120 + random.randint(-30, 30)  # å¢åŠ æ­¥é•·æé«˜æ•ˆç‡

            clicks_in_scroll, all_posts = self.fast_scroll_with_realtime_extract(
                total_distance=scroll_distance,
                step=step_size,
                all_posts=all_posts
            )
            total_see_more_clicks += clicks_in_scroll

            # è¨ˆç®—å®Œæ•´è²¼æ–‡æ•¸é‡
            complete_posts_for_display = [
                post for post in all_posts
                if not ('æŸ¥çœ‹æ›´å¤š' in post.get('post_text', '') or
                        'See More' in post.get('post_text', '') or
                        'See more' in post.get('post_text', ''))
            ]

            current_count = len(all_posts)
            complete_count = len(complete_posts_for_display)
            print(f"ğŸ“Š å·²æ“·å– {current_count} ç¯‡è²¼æ–‡ï¼ˆå…¶ä¸­ {complete_count} ç¯‡å®Œæ•´ï¼‰...")

            # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•ä¿å­˜ï¼ˆåŸºæ–¼å®Œæ•´è²¼æ–‡æ•¸é‡ï¼‰
            if complete_count - last_save_count >= self.auto_save_interval:
                # å–å¾—æ–°å¢çš„å®Œæ•´è²¼æ–‡
                new_complete_posts = complete_posts_for_display[last_save_count:]
                if new_complete_posts:
                    saved_file = self.save_partial_results(
                        new_complete_posts, batch_number)
                    if saved_file:
                        print(
                            f"ğŸ’¾ è‡ªå‹•ä¿å­˜å®Œæˆï¼šç¬¬ {batch_number} æ‰¹æ¬¡ï¼Œ{len(new_complete_posts)} ç¯‡å®Œæ•´è²¼æ–‡")
                        batch_number += 1
                        last_save_count = complete_count

            # æ›´æ–°é€²åº¦ï¼ˆåŸºæ–¼å®Œæ•´è²¼æ–‡æ•¸é‡ï¼‰
            if progress_callback:
                progress = min(100, (complete_count / max_posts) * 100)
                progress_callback(progress, complete_count)

            scroll_attempts += 1

            # æ¯10æ¬¡æ»¾å‹•å¾ŒçŸ­æš«æª¢æŸ¥
            if scroll_attempts % 10 == 0:
                print("ğŸ”„ å¿«é€Ÿæª¢æŸ¥éºæ¼çš„ã€ŒæŸ¥çœ‹æ›´å¤šã€...")
                extra_clicks, all_posts = self.quick_click_see_more(all_posts)
                total_see_more_clicks += extra_clicks
                if extra_clicks > 0:
                    print(f"âœ… å¿«é€Ÿæª¢æŸ¥é¡å¤–æ‰¾åˆ° {extra_clicks} å€‹æ–‡å­—æ¨™ç±¤ï¼Œå·²æ›´æ–°å…§å®¹")

                print(f"â¸ï¸ çŸ­æš«æš«åœ... (å·²æ»¾å‹• {scroll_attempts} æ¬¡)")
                time.sleep(random.uniform(1, 2))  # å¤§å¹…ç¸®çŸ­æš«åœæ™‚é–“

        # æœ€çµ‚æ¸…ç†ï¼šå¿«é€Ÿæª¢æŸ¥éºæ¼çš„ã€ŒæŸ¥çœ‹æ›´å¤šã€
        print("ğŸ§¹ æœ€çµ‚æ¸…ç†ï¼šå¿«é€Ÿæª¢æŸ¥éºæ¼çš„ã€ŒæŸ¥çœ‹æ›´å¤šã€...")
        final_clicks, all_posts = self.quick_click_see_more(all_posts)
        total_see_more_clicks += final_clicks
        if final_clicks > 0:
            print(f"âœ… æœ€çµ‚æ¸…ç†æ‰¾åˆ° {final_clicks} å€‹éºæ¼çš„æ–‡å­—æ¨™ç±¤ï¼Œå·²æ›´æ–°å…§å®¹")

        # ä¿å­˜å‰©é¤˜çš„å®Œæ•´è²¼æ–‡
        final_complete_posts = [
            post for post in all_posts
            if not ('æŸ¥çœ‹æ›´å¤š' in post.get('post_text', '') or
                    'See More' in post.get('post_text', '') or
                    'See more' in post.get('post_text', ''))
        ]

        if len(final_complete_posts) > last_save_count:
            remaining_posts = final_complete_posts[last_save_count:]
            if remaining_posts:
                saved_file = self.save_partial_results(
                    remaining_posts, batch_number)
                if saved_file:
                    print(f"âœ… æœ€çµ‚æ‰¹æ¬¡ä¿å­˜å®Œæˆï¼š{len(remaining_posts)} ç¯‡å®Œæ•´è²¼æ–‡")

        # ç¸½å…±é»æ“Šçš„ã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—æ¨™ç±¤çµ±è¨ˆ
        print(f"ğŸ“ˆ é«˜æ•ˆå®Œæˆï¼šå…±è™•ç†äº† {total_see_more_clicks} å€‹ã€ŒæŸ¥çœ‹æ›´å¤šã€æ–‡å­—æ¨™ç±¤ï¼Œå¯¦æ™‚æŠ“å–äº†å…§å®¹")

        # æœ€çµ‚é¸å–å®Œæ•´è²¼æ–‡
        print("ğŸ§¹ æœ€çµ‚é¸å–ï¼šæå–å®Œæ•´çš„è²¼æ–‡...")
        complete_posts = [
            post for post in all_posts
            if not ('æŸ¥çœ‹æ›´å¤š' in post.get('post_text', '') or
                    'See More' in post.get('post_text', '') or
                    'See more' in post.get('post_text', ''))
        ]

        # å–å¾—æŒ‡å®šæ•¸é‡çš„å®Œæ•´è²¼æ–‡
        final_posts = complete_posts[:max_posts]
        filtered_out_count = len(all_posts) - len(complete_posts)

        if filtered_out_count > 0:
            print(f"ğŸ“ å¾ {len(all_posts)} ç¯‡è²¼æ–‡ä¸­éæ¿¾æ‰ {filtered_out_count} ç¯‡æˆªæ–·è²¼æ–‡")
            print(f"ğŸ“Š æœ€çµ‚é¸å– {len(final_posts)} ç¯‡å®Œæ•´è²¼æ–‡")
        else:
            print(f"âœ… æ‰€æœ‰ {len(final_posts)} ç¯‡è²¼æ–‡éƒ½æ˜¯å®Œæ•´å…§å®¹")

        self.scraped_posts = final_posts

        # å¦‚æœæœ‰ä¿å­˜éƒ¨åˆ†æª”æ¡ˆï¼Œé€šçŸ¥ä½¿ç”¨è€…
        if self.partial_files:
            print(f"ğŸ“ å·²å»ºç«‹ {len(self.partial_files)} å€‹éƒ¨åˆ†æª”æ¡ˆï¼Œçˆ¬å–å®Œæˆå¾Œæœƒè‡ªå‹•åˆä½µ")
            if self.save_callback:
                self.save_callback(
                    f"å·²å»ºç«‹ {len(self.partial_files)} å€‹å‚™ä»½æª”æ¡ˆï¼Œå¯é¿å…è³‡æ–™ä¸Ÿå¤±")

        return self.scraped_posts

    def save_partial_results(self, posts_batch, batch_number):
        """å„²å­˜éƒ¨åˆ†çˆ¬å–çµæœ"""
        if not posts_batch:
            return None

        # éæ¿¾æˆªæ–·è²¼æ–‡å†ä¿å­˜
        filtered_batch = []
        filtered_count = 0

        for post in posts_batch:
            post_text = post.get('post_text', '')
            has_see_more = (
                'æŸ¥çœ‹æ›´å¤š' in post_text or
                'See More' in post_text or
                'See more' in post_text
            )

            if not has_see_more:
                filtered_batch.append(post)
            else:
                filtered_count += 1

        if filtered_count > 0:
            print(
                f"ğŸ“ éƒ¨åˆ†ä¿å­˜æ™‚éæ¿¾äº† {filtered_count} ç¯‡æˆªæ–·è²¼æ–‡ï¼Œä¿å­˜ {len(filtered_batch)} ç¯‡å®Œæ•´è²¼æ–‡")

        if not filtered_batch:
            print("âš ï¸ æœ¬æ‰¹æ¬¡æ²’æœ‰å®Œæ•´è²¼æ–‡å¯ä¿å­˜")
            return None

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_posts_partial_{timestamp}_batch_{batch_number}.csv"

            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['post_text', 'likes', 'comments',
                              'shares', 'post_time', 'post_url', 'scraped_at']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for post in filtered_batch:
                    writer.writerow(post)

            self.partial_files.append(filename)
            print(f"éƒ¨åˆ†è³‡æ–™å·²å„²å­˜è‡³: {filename}")

            # é€šçŸ¥GUIä¿å­˜ç‹€æ…‹
            if self.save_callback:
                self.save_callback(
                    f"å·²è‡ªå‹•ä¿å­˜ {len(filtered_batch)} ç¯‡å®Œæ•´è²¼æ–‡åˆ°: {filename}")

            return filename

        except Exception as e:
            print(f"å„²å­˜éƒ¨åˆ†CSVæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def merge_partial_files(self, final_filename=None):
        """åˆä½µæ‰€æœ‰éƒ¨åˆ†æª”æ¡ˆç‚ºæœ€çµ‚æª”æ¡ˆ"""
        if not self.partial_files:
            print("æ²’æœ‰éƒ¨åˆ†æª”æ¡ˆéœ€è¦åˆä½µ")
            return False

        if not final_filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            final_filename = f"facebook_posts_final_{timestamp}.csv"

        try:
            all_posts = []

            # è®€å–æ‰€æœ‰éƒ¨åˆ†æª”æ¡ˆ
            for partial_file in self.partial_files:
                if os.path.exists(partial_file):
                    with open(partial_file, 'r', encoding='utf-8-sig') as csvfile:
                        reader = csv.DictReader(csvfile)
                        for row in reader:
                            all_posts.append(row)

            # å»é™¤é‡è¤‡é …ç›®
            unique_posts = self.remove_duplicates(all_posts)

            # å¯«å…¥æœ€çµ‚æª”æ¡ˆ
            with open(final_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                if unique_posts:
                    fieldnames = unique_posts[0].keys()
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                    writer.writeheader()
                    for post in unique_posts:
                        writer.writerow(post)

            print(f"æœ€çµ‚åˆä½µæª”æ¡ˆå·²å„²å­˜è‡³: {final_filename}")
            print(f"ç¸½å…±åˆä½µäº† {len(unique_posts)} ç¯‡ç¨ç‰¹è²¼æ–‡")

            # æ¸…ç†éƒ¨åˆ†æª”æ¡ˆï¼ˆå¯é¸ï¼‰
            self.cleanup_partial_files()

            return final_filename

        except Exception as e:
            print(f"åˆä½µæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def cleanup_partial_files(self):
        """æ¸…ç†éƒ¨åˆ†æª”æ¡ˆ"""
        try:
            for partial_file in self.partial_files:
                if os.path.exists(partial_file):
                    os.remove(partial_file)
                    print(f"å·²æ¸…ç†éƒ¨åˆ†æª”æ¡ˆ: {partial_file}")
            self.partial_files = []
        except Exception as e:
            print(f"æ¸…ç†éƒ¨åˆ†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def save_to_csv(self, filename=None):
        """å°‡çˆ¬å–çš„è³‡æ–™å„²å­˜ç‚ºCSVæª”æ¡ˆ"""
        # å¦‚æœæœ‰éƒ¨åˆ†æª”æ¡ˆï¼Œå…ˆåˆä½µå®ƒå€‘
        if self.partial_files:
            merged_file = self.merge_partial_files(filename)
            if merged_file:
                return True

        # å¦‚æœæ²’æœ‰éƒ¨åˆ†æª”æ¡ˆä½†æœ‰å®Œæ•´çš„çˆ¬å–è³‡æ–™
        if not self.scraped_posts:
            print("æ²’æœ‰è³‡æ–™å¯ä¾›å„²å­˜")
            return False

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_posts_{timestamp}.csv"

        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['post_text', 'likes', 'comments',
                              'shares', 'post_time', 'post_url', 'scraped_at']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for post in self.scraped_posts:
                    writer.writerow(post)

            print(f"è³‡æ–™å·²å„²å­˜è‡³: {filename}")
            return True

        except Exception as e:
            print(f"å„²å­˜CSVæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def stop_scraping_process(self):
        """åœæ­¢çˆ¬å–éç¨‹"""
        self.stop_scraping = True
        print("æ­£åœ¨åœæ­¢çˆ¬å–éç¨‹...")

    def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.driver:
            self.driver.quit()
            print("ç€è¦½å™¨å·²é—œé–‰")
